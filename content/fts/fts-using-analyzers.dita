<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_o23_j34_1v">
  
  <title>
    Understanding Analyzers
  </title>
  
  <shortdesc>
    Analyzers increase search-awareness by transforming input text into token-streams, which
    permit the management of richer and more finely controlled forms of text-matching. An 
    analyzer consists of modules, each of
    which performs a particular role in the transformation (for example, removing
    undesirable characters; transforming dictionary-based words into non-dictionary-based
    search-tokens; and performing miscellaneous post-processing activities). 
  </shortdesc>
  
  <body>
  
  <section>
    <title>
      Principles of Text-Analysis
    </title>
  
    <p>
      <i>Analyzers</i> pre-process input-text submitted for Full Text Search; typically, by removing
      characters that might prohibit certain match-options. Analysis is performed on document-contents,
      when indexes are created; and is also performed on the input-text submitted for a search. The
      benefits of analysis are often referred to as <i>language awareness</i>.
    </p>
    
    <p>
      For example, if the input-text for a search is <codeph>watered down</codeph>, and the document-content
      contains the phrase <codeph>watery light beer</codeph>, the dictionary-based words do not permit
      a match. However, by using an analyzer that <i>stems</i> words, the input-text yields the tokens
      <codeph>water</codeph> and <codeph>down</codeph>; while the document-content yields the tokens
      <codeph>water</codeph>, <codeph>light</codeph>, and <codeph>beer</codeph>. This permits a match on
      <codeph>water</codeph>.
    </p>
    
    <p>
      Since different analyzers pre-process text in different ways, effective Full Text Search depends on the right choice
      of analyzer, for the type of matches that are desired.
    </p>
    
    <p>
      Couchbase Full Text Search provides a number of pre-constructed analyzers that can defined for use with
      Full Text Indexes. Additionally, analyzers can be custom-created, by means of the Couchbase Web Console. The 
      remainder of this page explains the architecture of analyzers, and describes the modular components that
      Couchbase Full Text Search makes available for custom-creation. It also lists the pre-constructed analyzers
      that are available, and describes the modules that they contain.
    </p>
      
  </section>
    
    <section>
      
      <title>
        Analyzer Architecture
      </title>

      <p>
        Analyzers are built up from modular components:
      </p>
        
        <ul>
          
          <li>
            <b>Character Filters</b>
            Strip out undesirable characters from input: for example, the <codeph>html</codeph> 
            character filter ignores HTML tags, and indexes HTML text-content alone.
          </li>
          
          <li>
            <b>Tokenizers</b>
            split input-strings into individual 
            <i>tokens</i>, which together are made into a <i>token stream</i>. Typically, 
            a token is established for each word.
          </li>
          
          <li>
            <b>Token Filters</b> are chained together, to perform additional post-processing on the token stream.
          </li>
          
        </ul> 
      
      <p>
        Each component-type is described below.
      </p>
    
    </section>
    
    
    <section>
      
      <title>
        Tokenizers
      </title>
      
      <p>
        Tokenizers split input-strings into individual tokens: characters likely to prohibit certain kinds
        of matching (for example, spaces or commas) are omitted. The tokens so created are then made into
        a <i>token stream</i> for the query.
      </p>
      

      
      <p>
        The following tokenizers are available from the Couchbase Web Console:
      </p>
      
      <ul>
        <li>
          <b>Letter</b>: Creates tokens by breaking input-text into subsets that consist of letters only:
          characters such as punctuation-marks and numbers are omitted. Creation of a token ends whenever
          a non-letter character is encountered. For example, the text <codeph>Reqmnt: 7-element phrase</codeph>
          would return the following tokens: <codeph>Reqmnt</codeph>, <codeph>element</codeph>, and 
          <codeph>phrase</codeph>.
          
          <p>
            
          </p>
          
        </li>
        
        <li>
          <b>Single</b>: Creates a single token from the entirety of the input-text. For example, the text
          <codeph>in each place</codeph> would return the following token: <codeph>in each place</codeph>.
          Note that this may be useful for handling URLs or email-addresses, which can thus be prevented
          from being broken at punctuation or special-character boundaries. It may also be used to prevent
          multi-word phrases (such as proper placenames, like <codeph>Milton Keynes</codeph> or
          <codeph>San Francisco</codeph>) from being broken up due to whitespace; so that they become indexed
          as a single term.
          
          <p>
            
          </p>
          
        </li>
        
        <li>
          <b>Unicode</b>: Creates tokens by performing Unicode Text Segmentation on word-boundaries. For
          examples, see
          <xref href="http:http://www.unicode.org/reports/tr29/#Word_Boundaries" scope="external" format="html">Unicode Word Boundaries</xref>.
        
          <p>
            This tokenizer uses the <xref href="https://github.com/blevesearch/segment" format="html" scope="external">segment</xref> library.
          </p>
          
        </li>
        
        <li>
          <b>Web</b>: Creates tokens by identifying and removing html tags. For example, the text
          <codeph>&lt;h1&gt;Introduction&lt;\h1&gt;</codeph> would return the token <codeph>Introduction</codeph>.
          
          <p>
            
          </p>
          
        </li>
        
        <li>
          <b>Whitespace</b>: Creates tokens by breaking input-text into subsets according to where
          whitespace occurs. For example, the text <codeph>in each place</codeph> would return the following
          tokens: <codeph>in</codeph>, <codeph>each</codeph>, and <codeph>place</codeph>.
          
          <p>
            
          </p>
          
        </li>
        
      </ul>
    
    </section>
    
    <section>
      
      <title>
        Token Filters
      </title>
      
      <p>
        <i>Token Filters</i> accept a token-stream provided by a tokenizer, and make modifications to the tokens in the stream.
      </p>
      
      <p>
        A frequently used form of token filtering is <i>stemming</i>; this reduces words
        to a base form that typically consists of the initial <i>stem</i> of the word (for example,
        <codeph>play</codeph>, which is the stem of <codeph>player</codeph>, <codeph>playing</codeph>,
        <codeph>playable</codeph>, and more). With the stem used as the token, a wider variety of matches
        can be made (for example, the input-text <codeph>player</codeph> can be matched with the document-content
        <codeph>playable</codeph>).
      </p>
      
      <p>
        The following kinds of token-filtering are supported by Couchbase Full Text Search:
      </p>
      
      <ul>
        
        <li>
          <b>apostrophe</b>: Removes all characters after an apostrophe, and the apostrophe itself. For example,
          <codeph>they've</codeph> becomes <codeph>they</codeph>.
          
          <p>
            
          </p>
        </li>
        
        <li>
          <b>elision</b>: Identifies and removes articles prefixing a term and separated by an apostrophe. For example,
          in French, <codeph>l'avion</codeph> becomes <codeph>avion</codeph>.
          
          <p>
            
          </p>
        </li>
        
        <li>
          <b>normalize</b>: Converts into 
          <xref href="http://unicode.org/reports/tr15/" scope="external" format="html">Unicode Normalization Form</xref>.
          
          <p>
            
          </p>
        </li>
        
        <li>
          <b>stemmer</b>: Uses 
          <xref href="http://snowball.tartarus.org/" scope="external" format="html">libstemmer</xref> to reduce tokens to word-stems.
          
          <p>
            
          </p>
        </li>
        
        <li>
          <b>stop</b>: Refers to a map of tokens that should be removed from the stream, due to their presumed unhelpfulness in the
          context of Full Text Search. The map consists of words such as <codeph>and</codeph>, <codeph>is</codeph>, and <codeph>the</codeph>.
          
          <p>
            
          </p>
        </li>
        
        <li>
          <b>to_lower</b>: Converts all characters in a token to lower case. For example, <codeph>HTML</codeph> becomes <codeph>html</codeph>.
          
          <p>
            
          </p>
        </li>
        
        <li>
          <b>possessive</b>: Removes possessives from tokens based on English words. For example, <codeph>software's</codeph> 
          becomes <codeph>software</codeph>.
          
          <p>
            
          </p>
        </li>
        
      </ul>
      
      <p>
        Note that token filters are frequently configured according to the special characteristics of individual languages.
        Couchbase Full Text Search provides multiple language-specific versions of the <b>elision</b>, <b>normalize</b>, 
        <b>stemmer</b>, and <b>stop</b> token filters. Specially supported languages include <b>ca</b> (Catalan), <b>fr</b> (French), 
        <b>ga</b> (Gaelic), 
        <b>it</b> (Italian), <b>ar</b> (Arabic), <b>ckb</b> (Sorani Kurdish), <b>fa</b> (Persian), <b>hi</b> (Hindi), 
        <b>in</b> (Indonesian), <b>en</b> (English), <b>cs</b> (Czech), <b>el</b> (Greek), <b>eu</b> (Basque),
        <b>hy</b> (Armenian), and <b>pt</b>(Portuguese). Additionally, token filters are provided for normalizing the width of and 
        forming bigrams from
        token based on <b>cjk</b> (Chinese, Japanese, and Korean).
      </p>
      
    </section>
    
    <section>
      
      <title>
        Customizing the Field Mappings
      </title>
      
      <p>
        <b>Dynamic Index Mapping</b> 
      </p> 
      
      <p>
        When the indexer encounters a field whose type you haven’t explicitly specified, it guesses
        the type by looking at the JSON value. 
      </p>
        
        <table frame="all" rowsep="1" colsep="1"
          id="table_dcs_gl4_1v">
          <tgroup cols="2" align="left">
            <colspec colname="c1" colnum="1" colwidth="1.0*"/>
            <colspec colname="c2" colnum="2" colwidth="1.0*"/>
            <thead>
              <row>
                <entry>Type of JSON value</entry>
                <entry>Indexed as...</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>Boolean</entry>
                <entry>Boolean</entry>
              </row>
              <row>
                <entry>Number</entry>
                <entry>Number</entry>
              </row>
              <row>
                <entry>String containing a date</entry>
                <entry>Date</entry>
              </row>
              <row>
                <entry>String (not containing a date)</entry>
                <entry>String</entry>
              </row>
            </tbody>
          </tgroup>
        </table> 
      
      <p>
        The indexer attempts to parse String values as dates and indexes them as such if
        the operation succeeds. Full text search and Bleve expect dates to be in the format
        specified by <xref href="https://www.ietf.org/rfc/rfc3339.txt" format="html"
          scope="external">RFC-3339</xref>, which is a specific profile of ISO-8601 that is more
        restrictive. 
      </p>
      
      <p>
        Note that other String values like "7" or "true" will never be indexed as a number or
        Boolean. The number type is modeled as 64-bit floating point value internally.
      </p>    
    
    </section>
    
  </body>
  
</topic>
